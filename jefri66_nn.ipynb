{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrungwaew/811project/blob/main/jefri66_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC7iRgrXVOpF"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "%pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from functools import partial\n",
        "from keras_tuner import RandomSearch, Objective\n",
        "\n",
        "nltk.download(\"treebank\")\n",
        "nltk.download(\"universal_tagset\")\n",
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q7LXP3YWodx",
        "outputId": "eaa10094-617d-44ff-e21b-8087d132ea73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sentences = treebank.tagged_sents(tagset=\"universal\")\n",
        "goje = pd.Series([len(tagged_sentence) for tagged_sentence in tagged_sentences])\n",
        "print(goje.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TumzXdLyq8e-",
        "outputId": "e136eef0-efaf-4d2c-adc5-b0afa0e913eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    3914.000000\n",
            "mean       25.722024\n",
            "std        12.868534\n",
            "min         1.000000\n",
            "25%        17.000000\n",
            "50%        25.000000\n",
            "75%        33.000000\n",
            "max       271.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tagged_sentence_df(tagged_sentence, max_words):\n",
        "    \"\"\"Docstring here.\"\"\"\n",
        "    tagged_sentence_df = pd.DataFrame(\n",
        "        [(word, pos) for word, pos in tagged_sentence],\n",
        "        columns=[\"word\", \"pos\"],\n",
        "        dtype=\"string\",\n",
        "    ).iloc[:max_words]\n",
        "    tagged_sentence_df = tagged_sentence_df.assign(\n",
        "        **{\n",
        "            \"first_word\": (tagged_sentence_df.index == 0).astype(\"float32\"),\n",
        "            \"last_word\": (tagged_sentence_df.index == len(tagged_sentence) - 1).astype(\n",
        "                \"float32\"\n",
        "            ),\n",
        "            \"first_upper\": tagged_sentence_df.word.str.istitle().astype(\"float32\"),\n",
        "            \"all_upper\": tagged_sentence_df.word.str.isupper().astype(\"float32\"),\n",
        "            \"is_num\": tagged_sentence_df.word.str.isnumeric().astype(\"float32\"),\n",
        "            \"hyphen_present\": tagged_sentence_df.word.str.contains(\"-\").astype(\n",
        "                \"float32\"\n",
        "            ),\n",
        "        }\n",
        "    )\n",
        "    num_missing = max_words - tagged_sentence_df.shape[0]\n",
        "    if num_missing == 0:\n",
        "        return tagged_sentence_df\n",
        "    else:\n",
        "        return pd.concat(\n",
        "            [\n",
        "                tagged_sentence_df,\n",
        "                pd.DataFrame(\n",
        "                    {\n",
        "                        col: [\"FILLIN\" for _ in range(num_missing)]\n",
        "                        for col in (\"word\", \"pos\")\n",
        "                    }\n",
        "                    | {\n",
        "                        col: [0 for _ in range(num_missing)]\n",
        "                        for col in tagged_sentence_df.columns\n",
        "                        if col not in (\"word\", \"pos\")\n",
        "                    }\n",
        "                ).astype(\n",
        "                    {col: \"string\" for col in (\"word\", \"pos\")}\n",
        "                    | {\n",
        "                        col: \"float32\"\n",
        "                        for col in tagged_sentence_df.columns\n",
        "                        if col not in (\"word\", \"pos\")\n",
        "                    }\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "def load_tagged_sentences(max_words):\n",
        "    \"\"\"Docstring here.\"\"\"\n",
        "    tagged_sentences = treebank.tagged_sents(tagset=\"universal\")\n",
        "    sentence_dfs_list = [\n",
        "        make_tagged_sentence_df(tagged_sentence, max_words)\n",
        "        for tagged_sentence in tagged_sentences\n",
        "    ]\n",
        "    return pd.concat(sentence_dfs_list)\n",
        "\n",
        "\n",
        "def format_for_keras(tagged_sentences_df, features_colnames, max_words):\n",
        "    \"\"\"Docstring here.\"\"\"\n",
        "    data_dict = {}\n",
        "\n",
        "    features = (\n",
        "        tagged_sentences_df[features_colnames]\n",
        "        .to_numpy()\n",
        "        .reshape((-1, max_words, len(features_colnames)))\n",
        "    )\n",
        "\n",
        "    split_index = int(round(features.shape[0] * 0.8))\n",
        "\n",
        "    data_dict[\"train_features\"], data_dict[\"test_features\"] = (\n",
        "        features[:split_index],\n",
        "        features[split_index:],\n",
        "    )\n",
        "\n",
        "    data_dict[\"nunique_labels\"] = tagged_sentences_df.pos.nunique()\n",
        "    labels = (\n",
        "        pd.get_dummies(tagged_sentences_df.pos, dtype=\"float32\")\n",
        "        .to_numpy()\n",
        "        .reshape((-1, max_words, data_dict[\"nunique_labels\"]))\n",
        "    )\n",
        "    data_dict[\"train_labels\"], data_dict[\"test_labels\"] = (\n",
        "        labels[:split_index],\n",
        "        labels[split_index:],\n",
        "    )\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "\n",
        "def build_model(nunique_labels, hp):\n",
        "    \"\"\"Docstring here.\"\"\"\n",
        "    model = Sequential()\n",
        "    # Tune the number of layers\n",
        "    for gru_layer in range(hp.Int(\"num_layers\", 1, 3)):\n",
        "        model.add(\n",
        "            GRU(\n",
        "                # Tune number of units separately\n",
        "                units=hp.Int(\n",
        "                    f\"units_{gru_layer}\",\n",
        "                    min_value=nunique_labels,\n",
        "                    max_value=100,\n",
        "                    step=10,\n",
        "                ),\n",
        "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
        "                return_sequences=True,\n",
        "            )\n",
        "        )\n",
        "    model.add(Dense(nunique_labels, activation=\"softmax\"))\n",
        "\n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def main():\n",
        "    max_words = 30\n",
        "    tagged_sentences_df = load_tagged_sentences(max_words)\n",
        "\n",
        "    features_colnames = [\n",
        "        col for col in tagged_sentences_df.columns if col not in (\"word\", \"pos\")\n",
        "    ]\n",
        "    data_dict = format_for_keras(tagged_sentences_df, features_colnames, max_words)\n",
        "    print(data_dict[\"train_features\"].shape)\n",
        "    print(data_dict[\"train_labels\"].shape)\n",
        "    hypermodel = partial(build_model, data_dict[\"nunique_labels\"])\n",
        "\n",
        "    tuner = RandomSearch(\n",
        "        hypermodel,\n",
        "        max_trials=10,\n",
        "        objective=Objective(\"val_accuracy\", \"max\"),\n",
        "    )\n",
        "    tuner.search(\n",
        "        data_dict[\"train_features\"],\n",
        "        data_dict[\"train_labels\"],\n",
        "        validation_data=(data_dict[\"test_features\"], data_dict[\"test_labels\"]),\n",
        "    )\n",
        "    print(tuner.results_summary())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7JS4DAPVSIX",
        "outputId": "31ae8c12-4a76-496d-8b1e-6b78efa249ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 08s]\n",
            "val_accuracy: 0.3145168125629425\n",
            "\n",
            "Best val_accuracy So Far: 0.4809706211090088\n",
            "Total elapsed time: 00h 01m 45s\n",
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7fd604e7d340>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 33\n",
            "activation: tanh\n",
            "lr: 0.0015498572359259769\n",
            "units_1: 53\n",
            "units_2: 43\n",
            "Score: 0.4809706211090088\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "units_0: 23\n",
            "activation: tanh\n",
            "lr: 0.0016431536744526529\n",
            "units_1: 33\n",
            "units_2: 33\n",
            "Score: 0.45913153886795044\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 83\n",
            "activation: tanh\n",
            "lr: 0.0010104015908082703\n",
            "Score: 0.4544912874698639\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "units_0: 13\n",
            "activation: relu\n",
            "lr: 0.0008166224340086715\n",
            "units_1: 23\n",
            "units_2: 93\n",
            "Score: 0.3629629611968994\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 63\n",
            "activation: relu\n",
            "lr: 0.0007134180666249963\n",
            "units_1: 33\n",
            "units_2: 93\n",
            "Score: 0.3145168125629425\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 13\n",
            "activation: tanh\n",
            "lr: 0.00012459337838415332\n",
            "units_1: 93\n",
            "units_2: 53\n",
            "Score: 0.29374203085899353\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 33\n",
            "activation: tanh\n",
            "lr: 0.000132640181394537\n",
            "units_1: 13\n",
            "units_2: 13\n",
            "Score: 0.29063430428504944\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 13\n",
            "activation: relu\n",
            "lr: 0.00016530629852860978\n",
            "units_1: 33\n",
            "units_2: 33\n",
            "Score: 0.2399318814277649\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R1YtI4TfrBRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}